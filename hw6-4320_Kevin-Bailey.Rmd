---
title: "STA 4320 Homework 6"
author: "Kevin Bailey"
date: "10/31/18"
output: html_document
---

# Question 3 ISLR 5.4.8
```{r}
#Libraries

library(boot) # For LOOCV and K-Fold CV
library(MASS) # For Boston dataset
```

## Solution.

### Part (a)
```{r}
set.seed(1)
x <- rnorm(100)
y <- x - 2*x^2 + rnorm(100)
```

n = 100, and p = 2. The model is $Y = X - 2X^2 + \epsilon$.

### Part (b)
```{r}
plot(x, y)

cat(min(x), max(x))
cat(min(y), max(y))
```

The shape of this plot is clearly that of a quadratic(although upside down from the typical $x^2$, still has the parabola shape). Most of the data seems centered around x = 0. Our x-values range from about -2.2 to 2.4 while our y-values range from roughly -12.68 to 2.33. 

### Part (c)
```{r}
set.seed(2)
df <- data.frame(x,y)

glm.fit1 <- glm(y ~ x)
glm.fit2 <- glm(y ~ poly(x, 2))
glm.fit3 <- glm(y ~ poly(x, 3))
glm.fit4 <- glm(y ~ poly(x, 4))

# Printing and assigning in same line to save space, not sure if we will use the err variables later or not though.
print(cv.err1 <- cv.glm(df, glm.fit1)$delta[1])
print(cv.err2 <- cv.glm(df, glm.fit2)$delta[1])
print(cv.err3 <- cv.glm(df, glm.fit3)$delta[1])
print(cv.err4 <- cv.glm(df, glm.fit4)$delta[1])
```

### Part (d)
```{r}
set.seed(3)
df <- data.frame(x,y)

glm.fit1 <- glm(y ~ x)
glm.fit2 <- glm(y ~ poly(x, 2))
glm.fit3 <- glm(y ~ poly(x, 3))
glm.fit4 <- glm(y ~ poly(x, 4))

# Printing and assigning in same line to save space like before, not sure if we will use the err variables later or not though.
print(cv.err1 <- cv.glm(df, glm.fit1)$delta[1])
print(cv.err2 <- cv.glm(df, glm.fit2)$delta[1])
print(cv.err3 <- cv.glm(df, glm.fit3)$delta[1])
print(cv.err4 <- cv.glm(df, glm.fit4)$delta[1])
```

Yes, the results are the same as LOOCV has no random component to it, we test on each individual data point eventually for our model so the order in which we pick doesn't really matter; results are the same.

### Part (e)
Model ii had the lowest LOOCV test error rate which is the quadratic polynomial$(Y = \beta_0 + \beta_1 X + \beta_2 X^2 + \epsilon)$. This makes sense because of the form of Y being quadratic as well.

### Part (f)
```{r}
# May not need all of these since values only change from fit1 to fit2 in terms of what's significant.
summary(glm.fit1)
summary(glm.fit2)
summary(glm.fit3)
summary(glm.fit4)
```

Yes, these results agree with conclusions from the cross-validation results as the third and fourth degree polynomial terms are seen as insignificant while the quadratic term and linear term are very significant.

# Question 3 


## Solution.

### Part (a)

```{r}
set.seed(17) # Like the book does it, will need to do in every code block though.

glm.fit <- glm(crim ~ ., data = Boston)
summary(glm.fit)

print(cv.err <- cv.glm(Boston, glm.fit)$delta[1])
print(cv.err10 <- cv.glm(Boston, glm.fit, K=10)$delta[1])
```

The **AIC is 3336.5**, the **LOOCV is 42.96874** and the **10-fold CV is 42.93207**.

DELT THIS
```{r}
se <- c(0)
for(i in 1:dim(Boston)[1]) {
  glm.fit <- glm(crim ~ ., data=Boston[-i, ])
  predicted <- predict(glm.fit, Boston[i, ])
  
  se <- se + (Boston[i, ] - predicted)^2
}
se <- se / dim(Boston)[1]
```

### Part (b)

**Age** has the highest p-value, so we will remove that one.

```{r}
set.seed(17)

ind <- (which(names(Boston)=="age"))
glm.fit <- glm(crim ~ ., data = Boston[,-ind])
summary(glm.fit)

print(aic <- summary(glm.fit)$aic)
print(cv.err <- cv.glm(Boston, glm.fit)$delta[1])
print(cv.err10 <- cv.glm(Boston, glm.fit, K=10)$delta[1])
```

**AIC = 3334.5**

**LOOCV = 42.96874**

**10-fold CV = 42.93207**

### Part (c)

```{r}
set.seed(17)
largest <- 0.1
data_a <- Boston

# Variables for part (d) that use this code.
############
low_aic <- aic
low_loocv <- cv.err
low_10 <- cv.err10
############ 

while(largest >= 0.1)
{
  data_a <- data_a[,-ind] # Removes "age" like above in part (b).
  glm.fit <- glm(crim ~ ., data = data_a)
  print(summary(glm.fit))
  largest <- 0  
  
  for(i in 1:nrow(summary(glm.fit)$coefficients))
  {
    p_val <- summary(glm.fit)$coefficients[i,4]
    if(p_val > largest)
    {
      largest <- p_val
      ind <- i
    }
  }
  
  aic <- summary(glm.fit)$aic
  cv.err <- cv.glm(Boston, glm.fit)$delta[1]
  cv.err10 <- cv.glm(Boston, glm.fit, K=10)$delta[1]
  
  cat("       AIC: ", aic, "\n")
  cat("     LOOCV: ", cv.err, "\n")
  cat("10-fold CV: ", cv.err10, "\n\n")
  
  if(aic < low_aic)
  {
    low_aic <- aic
  }
  if(cv.err < low_loocv)
  {
    low_loocv <- cv.err
  }
  if(cv.err10 < low_10)
  {
    low_10 <- cv.err10
  }
}

```

I print the AIC, LOOCV, and 10-fold CV together after each summary. It might be hard to find? Summaries are printed to show what is being removed with each iteration of the loop.  I'll print here as well in the order they appear after removal of highest p-values(assuming *age* already removed).

AIC:  3335.028,
LOOCV:  42.96874, 
10-fold CV:  42.93207 

AIC:  3333.077, 
LOOCV:  42.96874, 
10-fold CV:  42.8935 

AIC:  3331.502, 
LOOCV:  42.96874, 
10-fold CV:  42.73634 

AIC:  3329.974, 
LOOCV:  42.96874, 
10-fold CV:  42.4803

AIC:  3329.8, 
LOOCV:  42.96874, 
10-fold CV:  43.50741

AIC:  3330.373, 
LOOCV:  42.96874, 
10-fold CV:  42.71068 

### Part (d)

```{r}
low_aic
low_loocv
low_10
```

The values in the previous block are from the loops in part (c). We can see the lowest values are as follows:

**AIC: 3329.8**

**LOOCV: 42.96874**

**10-fold CV: 42.4803**

From the above we can see the LOOCV values never change for our models(which makes sense because we have 7 predictors remaining after maxmimum removals, removing one more may have changed the LOOCV), so we can look strictly at the AIC and 10-fold CV. We also are not looking at higher degree polynomial models so interpretability isn't a *huge* concern. 

The lowest AIC value(3329.8) came from the second to last model where the corresponding 10-fold CV value was 43.50741.

The lowest 10-fold CV value(42.4803) came from the third to last model where the corresponding AIC value was 3329.974. 

Because of the AIC only being .174 away from the lowest value in the third model where the 10-fold CV value is the lowest(and the gap is larger to the 10-fold CV value in the model with the lowest AIC value), I would choose the third to last model. This is the model using *zn, indus, nox, dis, rad, ptratio, black, lstat, * and *medv* as the predictors. 

Hopefully this came out correct! I know we can eyeball it, but if we have a large data set that makes it hard to see and do some simple queries on I figured this way would be *somewhat* useful, even though it takes a few seconds for it to run haha. 