---
title: "STA 4320 Homework 8"
author: "John Doe"
date: "12/5/18"
output: html_document
---

# Question 6 ISLR 5.4.5


## Solution.

### Part (b)

#### i) 
```{r}
set.seed(37128)

library(ISLR)
# proprtion divided into training and test sets
fractionTraining <- 0.5
fractionTesting <- 0.5

# gather sample size for training and test sets
nTraining <- floor(fractionTraining*nrow(Default))
nTest <- floor(fractionTesting*nrow(Default))

# find indices for training and test sets
indicesTraining <- sort(sample(1:nrow(Default),size=nTraining))
indicesTesting <- setdiff(1:nrow(Default), indicesTraining)

# FINISH...

train.set <- Default[indicesTraining, ]
test.set <- Default[indicesTesting, ]

# Making sure of no overlap:
dim(train.set)
dim(test.set)

intersect(train.set, test.set)
dim(setdiff(test.set, train.set)) # Should be same size as the sets
```

#### ii)
```{r}
fit <- glm(default ~ income + balance, data=train.set, family="binomial")
summary(fit)
```

#### iii)
```{r}
predictions <- rep("No", nTraining)
px <- predict(fit, test.set, type="response")
predictions[px > 0.5] <- "Yes"

table(test.set$default, predictions) # Checking predictions vs actual
```

#### iv)
```{r}
1-mean(test.set$default == predictions) # Subtract from 1 to get % wrong.
```

### Part (d)
```{r}
fit <- glm(default ~ income + balance + student, data=train.set, family="binomial")

summary(fit)

predictions <- rep("No", nTraining)
px <- predict(fit, test.set, type="response")
predictions[px > 0.5] <- "Yes"

table(test.set$default, predictions) # Checking predictions vs actual again

1-mean(test.set$default == predictions)
```

The reduction of error rate with the dummy variable(0.028 to 0.0276) is extremely minor, so I would say there isn't really a difference in including the dummy variable for *student* or not.

# Question 7

### Part (a) 

```{r}
set.seed(37128)

library(ISLR)
library(boot) # cv
library(MASS) # lda/qda

# proprtion divided into training and test sets
fractionTraining <- 0.5
fractionTesting <- 0.5

# gather sample size for training and test sets
nTraining <- floor(fractionTraining*nrow(Default))
nTest <- floor(fractionTesting*nrow(Default))

# find indices for training and test sets
indicesTraining <- sort(sample(1:nrow(Default),size=nTraining))
indicesTesting <- setdiff(1:nrow(Default), indicesTraining)

# FINISH...

train.set <- Default[indicesTraining, ]
test.set <- Default[indicesTesting, ]

fit.lda <- lda(default ~ income + balance, data=train.set, family="binomial")

fit.qda <- qda(default ~ income + balance, data=train.set, family="binomial")

#lda
predictions <- rep("No", nTraining)
px <- predict(fit.lda, test.set, type="response")
predictions[px$posterior[,2] > 0.5] <- "Yes"
1-mean(test.set$default == predictions)

table(test.set$default, predictions)

#qda
predictions <- rep("No", nTraining) # resetting predictions
px <- predict(fit.qda, test.set, type="response")
predictions[px$posterior[,2] > 0.5] <- "Yes"
1-mean(test.set$default == predictions)


table(test.set$default, predictions)
```

### Part (b)
```{r}
fit <- glm(default ~ ., data=Default, family="binomial")

sum.err <- c(0)
for(i in 1:20){
  sum.err[i] <- cv.glm(Default, fit, K=10)$delta[1]
}
sum.err
cat("Mean of the errors: ", mean(sum.err))
```

### Part (c)
I ran the 10-fold CV 20 times and all of them were around 0.0214, while our other prediction errors were around 0.028. This is only less than 1% to put things into perspective, but the error for 10-fold CV does seem to be lower regularly.

### Part (d)
The Logistic Regression, LDA, and QDA errors were all extremely close. LDA had the highest error at 0.0294, but they all seemed to be around the 0.028 range. 