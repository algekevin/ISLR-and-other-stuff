---
title: "STA 4320 Homework 4"
author: "John Doe"
date: "10/3/18"
output: html_document
---

# Question 4 ISLR 3.7.10


## Solution.

### Part (a)

```{r}
library(ISLR)
```

```{r}
carseats.lm <- lm(Sales ~ Price + Urban + US, data=Carseats)
summary(carseats.lm)
```

### Part (b)
#### Price
There *is* a linear relationship between **sales** and **price** as we can see from the low p-value. The estimate shows a slightly negative relationship though, meaning as prices increase, the sales will decrease. 

#### UrbanYes
UrbanYes indicates that the store is in an Urban location rather than rural. The extremely high p-value here suggests that there *isn't* a relationship between being in an urban location and the sales of child carseats. 

#### USYes
USYes indicates that the store is located in the US. The small p-value suggests that there *is* a relationship between being **located in the US** and **sales**. This relationship is positive as can be seen by the estimate, which means that if the store is located in the US the sales will be higher. 

### Part (c)
$$Sales = \beta_0 + \beta_1Price + \beta_2UrbanYes + \beta_3USYes$$
So filling everything in we get:

$$Sales = 13.043 - 0.054Price -0.022UrbanYes + 1.2USYes$$
Note that UrbanYes and USYes are only included in the case the store is in the locations specified.

### Part (d)
We can reject the null hypothesis $H_0: \beta = 0$ for both **price** and **USYes** as their p-values are small enough to indicate this.

### Part (e)
```{r}
carseats.lm2 <- lm(Sales ~ Price + US, data=Carseats)
summary(carseats.lm2)
```

### Part (f)
Looking at the $R^2$ and the RSE we can see that both models from parts (a) and (e) fit the data very similarly. This should make sense because *Urban* showed to be statistically insignificant in part (a). The $R^2$ value in part (e) is very slightly higher, so it's a bit better of a fit for the data than the model from part (a) is.

### Part (g)
```{r}
confint(carseats.lm2)
```


# Question 5


## Solution.

### Part (a)
#### Forward model selection

I run linear regressions with just one predictor first to get the highest $R^2$ and I store that into the variable I call **r2** at the end, then repeat until we get to a set where no $R^2$ is better, then we stop.

One predictor:
```{r}
summary(lm(mpg ~ cylinders, data=Auto))$adj.r.squared
summary(lm(mpg ~ displacement, data=Auto))$adj.r.squared
summary(lm(mpg ~ horsepower, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight, data=Auto))$adj.r.squared
summary(lm(mpg ~ acceleration, data=Auto))$adj.r.squared
summary(lm(mpg ~ year, data=Auto))$adj.r.squared
summary(lm(mpg ~ origin, data=Auto))$adj.r.squared
r2 <- 0.6918 # Adjusted R^2 from 'weight'
```

Now doing the same but with two predictors:
```{r}
summary(lm(mpg ~ weight + cylinders, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + displacement, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + horsepower, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + acceleration, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + origin, data=Auto))$adj.r.squared
r2 <- .8072 # from weight + year
```

Now with three predictors:
```{r}
summary(lm(mpg ~ weight + year + cylinders, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + displacement, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + horsepower, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + acceleration, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin, data=Auto))$adj.r.squared
r2 <- 0.816 # from weight + year + origin
```

Four predictors:
```{r}
summary(lm(mpg ~ weight + year + origin + cylinders, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + displacement, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + horsepower, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + acceleration, data=Auto))$adj.r.squared
r2 <- 0.8162 # From any of the last three.
```

Five predictors:
```{r}
summary(lm(mpg ~ weight + year + origin + displacement + cylinders, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + displacement + horsepower, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + displacement + acceleration, data=Auto))$adj.r.squared
r2 <- 0.8177 # weight + year + origin + displacement + horsepower
```

Six predictors:
```{r}
summary(lm(mpg ~ weight + year + origin + displacement + horsepower + cylinders, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + displacement + horsepower + acceleration, data=Auto))$adj.r.squared
r2 <- 0.8184 # weight + year + origin + displacement + horsepower + cylinders
```

All predictors: 
```{r}
summary(lm(mpg ~ weight + year + origin + displacement + horsepower + cylinders + acceleration, data=Auto))$adj.r.squared
```

This final model gives an $R^2$ value of 0.8182 which is  not larger than the previous $R^2$ value of .8184. 

Thus, using the forward model selection process we get the predictors **Weight, Year, Origin, Displacement, Horsepower, and Cylinders** yielding the highest $R^2$ value via the multiple linear regression.

### Part (b)
####Backward model selection

All predictors:
```{r}
summary(lm(mpg ~ weight + year + origin + displacement + horsepower + cylinders + acceleration, data=Auto))$adj.r.squared
r2 <- .8182
```

Six predictors:
```{r}
summary(lm(mpg ~ year + origin + displacement + horsepower + cylinders + acceleration, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + displacement + horsepower + cylinders + acceleration, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + horsepower + cylinders + acceleration, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + displacement + cylinders + acceleration, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + displacement + horsepower + acceleration, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + displacement + horsepower + cylinders, data=Auto))$adj.r.squared

r2 <- 0.8184 # weight + year + origin + displacement + horsepower + cylinders
```

Five Predictors:
```{r}
summary(lm(mpg ~ weight + year + origin + displacement + horsepower, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + displacement + cylinders, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + origin + horsepower + cylinders, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + year + displacement + horsepower + cylinders, data=Auto))$adj.r.squared
summary(lm(mpg ~ weight + origin + displacement + horsepower + cylinders, data=Auto))$adj.r.squared
```

None of these have a higher adjusted $R^2$ value than when we used six predictors, so I believe we can stop here.

This means the predictors **Weight, Year, Origin, Displacement, Horsepower, and Cylinders** yield the highest $R^2$ value of 0.8184.