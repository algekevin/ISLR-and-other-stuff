---
title: "STA 4320 Homework 5"
author: "Kevin Bailey"
date: "10/15/18"
output: html_document
---

# Question 2 ISLR 3.7.9


## Solution.

### Part (a)

```{r}
library(ISLR)
library(MASS) # for studres()
plot(Auto)
```

### Part (b)
```{r}
cor(subset(Auto, select = -name))
```

### Part (c)
```{r}
summary(lm(mpg ~ .-name, data=Auto))
```

#### Part (i)
There is definitely a relationship between the predictors and *mpg* which can be seen by the F-statistic and it's corresponding p-value, as well as how close the $R^2$ value is to 1. 

#### Part (ii)
Just by looking at the p-values we can see that all of the predictors are statistically significant except for **cylinders, horsepower, and acceleration**.

#### Part (iii)
This positive coefficient for *year* indicates that each year the mpg increases by ~0.75, so each year cars are becoming slightly more fuel efficient.

### Part (d)
```{r}
par(mfrow = c(2,2))
plot(lm(mpg ~ .-name, data=Auto))
par(mfrow = c(1,1))
plot(studres(lm(mpg ~ .-name, data=Auto)))
```

The fit doesn't seem to be very accurate because there is a pattern to the residuals plot which may be quadratic. The Residuals vs Leverage plot seems to show some points with high leverage(or low, rather) like point 14. This point doesn't seem to be an outlier though based on the studres plot, whereas the outliers below Cook's Distance range don't seem to have high leverage, so they aren't very dangerous. 

Based on the studres() plot however, we can see some outliers above $t_i = 3$ and maybe below $t_i = -3$, so let's check:

```{r}
which(studres(lm(mpg ~ .-name, data=Auto)) >= 3)
which(studres(lm(mpg ~ .-name, data=Auto)) <= -3)
```

From this, we can see there are actually no points below $t_i = -3$ but there are quite a few above $t_i = 3$. Those entries above are the outliers.

### Part (e)
```{r}
summary(lm(mpg ~ .*. -name*.+.-name, data=Auto))
```

I think this way is correct? This tests all predictors and all combinations of interaction terms. Doing it like this gives a lot of interactions that are statistically insignificant. The significant ones are displacement:year, acceleration:year, and acceleration:origin.

### Part (f)
```{r}
par(mfrow = c(2,2))
plot(log(Auto$displacement), Auto$mpg)
plot(sqrt(Auto$displacement), Auto$mpg)
plot((Auto$displacement)^2, Auto$mpg)
```

```{r}
par(mfrow = c(2,2))
plot(log(Auto$weight), Auto$mpg)
plot(sqrt(Auto$weight), Auto$mpg)
plot((Auto$weight)^2, Auto$mpg)
```


# Question 3 ISLR 3.7.13


## Solution.
```{r}
#par(mfrow = c(1,1))
set.seed(1)
```

### Part (a)
```{r}
x <- rnorm(100)
```

### Part (b)
```{r}
eps <- rnorm(100, 0, .5) # 0.5 as sd because variance needs to be 0.25
```

### Part (c)
```{r}
y <- -1 + .5*x + eps
length(y)
```

The length of the vector *y* is 100. $\beta_0 = -1$ and $\beta_1 = 0.5$.

### Part (d)
```{r}
plot(x, y)
```

The relationship between x and y seems to be linear with a positive slope and some variance to it.

### Part (e)
```{r}
fit.lm <- lm(y ~ x)
summary(fit.lm)
```

The values of $\hat{\beta_0}$ and $\hat{\beta_1}$ are very close to the values of $\beta_0$ and $\beta_1$ from above. We also see the p-values of the two predictors in this model being extremely close to 0 so we can reject the null.

### Part (f)
```{r}
plot(x,y)
abline(fit.lm, col="blue")
abline(-1, 0.5, col="red")
legend("bottomright", c("Model Fit", "Pop. Regression"), col=c("blue", "red"), lwd=2)
```

### Part (g)
```{r}
fit.lm2 <- lm(y ~ x + I(x^2))
summary(fit.lm2)
```

It's kind of weird. The model has increased I believe because of the $R^2$ being higher, but the p-value of the $x^2$ term suggests there isn't a relationship there. 

### Part (h)
```{r}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, 0, .1)
y <- -1 + 0.5*x + eps

plot(x, y)
fit.lmh <- lm(y ~ x)
summary(fit.lmh)

abline(fit.lmh, col="blue")
abline(-1, 0.5, col="red")
legend("bottomright", c("Model Fit", "Pop. Regression"), col=c("blue", "red"), lwd=2)
```

Since we have a lower variance in our $\epsilon$ term, the $R^2$ values increased considerably and that's to be expected. We can also see that the model fit and population regression lines are overlapping which is evidence of very little noise. 

### Part (i)
```{r}
set.seed(1)
x <- rnorm(100)
eps <- rnorm(100, 0, 5)
y <- -1 + 0.5*x + eps

plot(x, y)
fit.lmi <- lm(y ~ x)
summary(fit.lmi)

abline(fit.lmi, col="blue")
abline(-1, 0.5, col="red")
legend("bottomright", c("Model Fit", "Pop. Regression"), col=c("blue", "red"), lwd=2)
```

As expected, this looks terrible. There is a ton of variance in $\epsilon$ making it hard to fit a line well. Just look at the $R^2$ value again and it's easy to see how poor it is with the value being so close to 0. One thing to also note however is that the two lines are still close together but their distance is more wide than previous models. 

### Part (j)
Original data set:
```{r}
confint(fit.lm)
```

Less noisy data set:
```{r}
confint(fit.lmh)
```

More noisy data set:
```{r}
confint(fit.lmi)
```

We can see that the noisiest data set has the widest intervals while the less noisy data set has the most narrow intervals. Another thing to see is that *all* of these models seem to have their x centered around 0.5.

# Question 4 ISLR 3.7.14


## Solution.

### Part (a)

```{r}
set.seed(1)
x1 <- runif(100)
x2 <- 0.5 * x1 + rnorm(100)/10
y <- 2 + 2*x1 + 0.3*x2 + rnorm(100)
```

The *form* of the linear model is $$Y = 2 + 2X_1 + 0.3X_2 + \epsilon$$

with $\epsilon$ ~ $N(0,1)$.

The regression coefficients are $\beta_0 = 2, \beta_1 = 2, \beta_3 = 0.3$.

### Part (b)
The correlation between $x_1$ and $x_2$ is:
```{r}
cor(x1, x2)
plot(x1, x2)
```

### Part (c)
```{r}
fit.lm <- lm(y ~ x1 + x2)
summary(fit.lm)
```

$\hat{\beta_0} = 2.13, \hat{\beta_1} = 1.44, \hat{\beta_2} = 1.01$

Of these values, only $\hat{\beta_0}$ is close to it's corresponding $\beta$ value, $\beta_0$. The p-value for $\hat{\beta_1}$ is slightly less than 0.5, so we may reject the null for this value. But the value for $\hat{\beta_2}$ is high enough to fail rejection of the null in that case. 

### Part (d)
```{r}
fit.lmd <- lm(y ~ x1)
summary(fit.lmd)
```

The p-value here for $\hat{\beta_1}$ is extremely small so we can reject the null, this is quite different from the $\hat{\beta_1}$ value when we used the model with both $\hat{\beta_1}, \hat{\beta_2}$ as the predictors. 

### Part (e)
```{r}
fit.lme <- lm(y ~ x2)
summary(fit.lme)
```

Again we can reject the null safely here as the p-value is so small, so $\hat{\beta_2}$ is very significant. Similar to the last problem, this is far different from the model that was using both predictors.

### Part (f)
The results are not contradictory because $\hat{\beta_1}, \hat{\beta_2}$ have very high correlation(could we say they're collinear? I think so), so it is hard to distinguish their effects when they are used for regression in tandem because they share so much information. However, as we saw in the previous parts, when we use them for regression separately the relationship between each individual predictor and y is more clear and each is significant.

### Part (g)
```{r}
x1 <- c(x1, 0.1)
x2 <- c(x2, 0.8)
y <- c(y, 6)

cor(x1, x2)

fit.lmg1 <- lm(y ~ x1 + x2)
summary(fit.lmg1)

fit.lmg2 <- lm(y ~ x1)
summary(fit.lmg2)

fit.lmg3 <- lm(y ~ x2)
summary(fit.lmg3)

par(mfrow = c(2,2))
plot(fit.lmg1)
plot(fit.lmg2)
plot(fit.lmg3)

plot(studres(fit.lmg1))
plot(studres(fit.lmg2))
plot(studres(fit.lmg3))
```

```{r}
which(studres(fit.lmg1) >= 3)
which(studres(fit.lmg1) <= -3)

which(studres(fit.lmg2) >= 3)
which(studres(fit.lmg2) <= -3)

which(studres(fit.lmg3) >= 3)
which(studres(fit.lmg3) <= -3)
```

In the model with both predictors we can see $\hat{\beta_1}$ becoming insignificant and $\hat{\beta_2}$ becoming significant which is the opposite of when we used both predictors in the previous model before adding in the new values(reference part **c**). The correlation between $\hat{\beta_1}$ and $\hat{\beta_2}$ is still somewhat high, but definitely lower than before so the "swapping" of the significances in the model with both predictors shouldn't be a huge deal.

Only the model using $\hat{\beta_1}$ as a predictor has an outlier based on the studres values. This outlier isn't necessarily a high leverage point for this specific model based on the Leverage plot, but in the other two models it is most definitely a high leverage point, yet it is not an outlier.

# Question 5 ISLR 3.7.15


## Solution.

### Part (a)

```{r}
library(MASS)
```

```{r}
zone.fit <- lm(crim ~ zn, data=Boston)
summary(zone.fit)

indus.fit <- lm(crim ~ indus, data=Boston)
summary(indus.fit)

chas.fit <- lm(crim ~ chas, data=Boston)
summary(chas.fit)

nox.fit <- lm(crim ~ nox, data=Boston)
summary(nox.fit)

rm.fit <- lm(crim ~ rm, data=Boston)
summary(rm.fit)

age.fit <- lm(crim ~ age, data=Boston)
summary(age.fit)

dis.fit <- lm(crim ~ dis, data=Boston)
summary(dis.fit)

rad.fit <- lm(crim ~ rad, data=Boston)
summary(rad.fit)

tax.fit <- lm(crim ~ tax, data=Boston)
summary(tax.fit)

ptratio.fit <- lm(crim ~ ptratio, data=Boston)
summary(ptratio.fit)

black.fit <- lm(crim ~ black, data=Boston)
summary(black.fit)

lstat.fit <- lm(crim ~ lstat, data=Boston)
summary(lstat.fit)

medv.fit <- lm(crim ~ medv, data=Boston)
summary(medv.fit)
```

There is a significant relationship between predictor and response for all of these except for *chas*. We can also check residual plots here to double check, but that will make this *very* lengthy and messy, and if I put too many plots in a window it would be harder to see things clearly. The p-values from the summaries should be enough, I think.

### Part (b)
```{r}
all.fit <- lm(crim ~., data=Boston)
summary(all.fit)
```

Looking at the p-values of this summary, we can reject the null for **zn, dis, rad, black,** and **medv**.

### Part (c)
```{r}
x <- c(coefficients(zone.fit)[2],
      coefficients(indus.fit)[2],
      coefficients(chas.fit)[2],
      coefficients(nox.fit)[2],
      coefficients(rm.fit)[2],
      coefficients(age.fit)[2],
      coefficients(dis.fit)[2],
      coefficients(rad.fit)[2],
      coefficients(tax.fit)[2],
      coefficients(ptratio.fit)[2],
      coefficients(black.fit)[2],
      coefficients(lstat.fit)[2],
      coefficients(medv.fit)[2])

y <- coefficients(all.fit)[2:14]
plot(x, y)
```

Between the simple and multiple regression coefficients there is a difference due to the simple case having the slope terms ignore every other predictor aside from itself, while the multiple case holds other predictors fixed. Just look at the correlation between all the predictors aside from chas and you can see these relationships.

```{r}
cor(Boston[-c(1,4)]) # Ignoring chas as well as our response variable.
```

### Part (d)
```{r}
zone2.fit <- lm(crim ~ poly(zn, 3), data=Boston)
summary(zone2.fit)

indus2.fit <- lm(crim ~ poly(indus, 3), data=Boston)
summary(indus2.fit)

# Have to leave out chas
#chas2.fit <- lm(crim ~ poly(chas, 3), data=Boston)
#summary(chas2.fit)

nox2.fit <- lm(crim ~ poly(nox, 3), data=Boston)
summary(nox2.fit)

rm2.fit <- lm(crim ~ poly(rm, 3), data=Boston)
summary(rm2.fit)

age2.fit <- lm(crim ~ poly(age, 3), data=Boston)
summary(age2.fit)

dis2.fit <- lm(crim ~ poly(dis, 3), data=Boston)
summary(dis2.fit)

rad2.fit <- lm(crim ~ poly(rad, 3), data=Boston)
summary(rad2.fit)

tax2.fit <- lm(crim ~ poly(tax, 3), data=Boston)
summary(tax2.fit)

ptratio2.fit <- lm(crim ~ poly(ptratio, 3), data=Boston)
summary(ptratio2.fit)

black2.fit <- lm(crim ~ poly(black, 3), data=Boston)
summary(black2.fit)

lstat2.fit <- lm(crim ~ poly(lstat, 3), data=Boston)
summary(lstat2.fit)

medv2.fit <- lm(crim ~ poly(medv, 3), data=Boston)
summary(medv2.fit)
```

With respect to the cubic coefficient, **zn, rm, rad, tax, lstat** are not significant. Whereas for **indus, nox, age, dis, ptratio, medv** they are. However, if you look at **black**, both the cubic *and* quadratic coefficients are insignificant, but the linear term for **black** *is* significant, meaning you can't really tell the difference with the quadratic and cubic terms for this predictor.


